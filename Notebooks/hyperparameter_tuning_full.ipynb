{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bd180cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Log fayl manzili\n",
    "log_path = r\"C:\\Users\\Rasulbek907\\Desktop\\Project_MP\\Log\\tuning.log\"\n",
    "\n",
    "# Log sozlamalari\n",
    "logging.basicConfig(\n",
    "    filename=log_path,\n",
    "    filemode='a',  # Append mode\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "try:\n",
    "    logging.info(\"CSV fayl o'qilmoqda:...\")\n",
    "    df = pd.read_csv(r\"C:\\Users\\Rasulbek907\\Desktop\\Project_MP\\Data\\Sampling_Data\\Smote_dataset.csv\")\n",
    "    logging.info(f\"Fayl muvaffaqiyatli o'qildi. SatÄ±rlar soni: {len(df)} ustunlar soni: {len(df.columns)}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"CSV faylni o'qishda xatolik: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cbb85bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19350 entries, 0 to 19349\n",
      "Data columns (total 21 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Unnamed: 0           19350 non-null  int64  \n",
      " 1   id                   19350 non-null  float64\n",
      " 2   ref                  19350 non-null  float64\n",
      " 3   subtitle             19350 non-null  float64\n",
      " 4   creatorname          19350 non-null  float64\n",
      " 5   totalbytes           19350 non-null  float64\n",
      " 6   lastupdated          19350 non-null  float64\n",
      " 7   downloadcount        19350 non-null  float64\n",
      " 8   title                19350 non-null  float64\n",
      " 9   tags                 19350 non-null  float64\n",
      " 10  anomaly              19350 non-null  float64\n",
      " 11  download_view_ratio  19350 non-null  float64\n",
      " 12  num_tags             19350 non-null  float64\n",
      " 13  title_length         19350 non-null  float64\n",
      " 14  subtitle_length      19350 non-null  float64\n",
      " 15  creator_popularity   19350 non-null  float64\n",
      " 16  owner_popularity     19350 non-null  float64\n",
      " 17  lastupdated_month    19350 non-null  float64\n",
      " 18  lastupdated_day      19350 non-null  float64\n",
      " 19  lastupdated_weekday  19350 non-null  float64\n",
      " 20  cluster              19350 non-null  int64  \n",
      "dtypes: float64(19), int64(2)\n",
      "memory usage: 3.1 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d21edb",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb846721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Logistic Regression natijalari:\n",
      "Accuracy: 0.9795865633074935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# X va y ajratish\n",
    "X = df.drop(columns=\"cluster\")\n",
    "y = df[\"cluster\"]\n",
    "\n",
    "# Train / Test ajratish\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Model yaratish\n",
    "log_model = LogisticRegression(max_iter=500, random_state=42)\n",
    "log_model.fit(X_train, y_train)\n",
    "\n",
    "# Bashorat\n",
    "y_pred = log_model.predict(X_test)\n",
    "\n",
    "# Baholash\n",
    "print(\"ðŸ“Š Logistic Regression natijalari:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae3b151",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c39072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ³ Decision Tree Classifier natijalari:\n",
      "Accuracy: 0.9984496124031008\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# X va y ajratish\n",
    "X = df.drop(columns=\"cluster\")\n",
    "y = df[\"cluster\"]\n",
    "\n",
    "# Train / Test ajratish\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Model yaratish (default sozlamalar bilan)\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "# Oâ€˜qitish\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Bashorat\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Natijalar\n",
    "print(\"ðŸŒ³ Decision Tree Classifier natijalari:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750a1a88",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "857c405f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9992248062015504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = df.drop(columns=\"cluster\")\n",
    "y = df[\"cluster\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "rf_model = RandomForestClassifier()  # default sozlamalar\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170c318b",
   "metadata": {},
   "source": [
    "# XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2992965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [10:54:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9989664082687338\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Ma'lumotlarni tayyorlash\n",
    "X = df.drop(columns=\"cluster\")\n",
    "y = df[\"cluster\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# XGBoost modelini yaratish\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,       # daraxtlar soni\n",
    "    learning_rate=0.1,      # oâ€˜rganish tezligi\n",
    "    max_depth=6,            # har bir daraxt maksimal chuqurligi\n",
    "    random_state=42,\n",
    "    use_label_encoder=False, # XGBoost >=1.6 uchun kerak\n",
    "    eval_metric='mlogloss'   # koâ€˜p klassli klassifikatsiya uchun\n",
    ")\n",
    "\n",
    "# Modelni fit qilish\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Bashorat qilish\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Natijani baholash\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182921f7",
   "metadata": {},
   "source": [
    "# HYPERPARAMETER TUNING "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af3574b",
   "metadata": {},
   "source": [
    "# MANUAL SEARCH "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f1f71",
   "metadata": {},
   "source": [
    "#  RandomForestClassifier + Manual Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3381922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ² Random Forest (Manual Search) Accuracy: 0.9330749354005168\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# ðŸ§© Model (qoâ€˜lda hyperparameterlar bilan)\n",
    "rf_manual = RandomForestClassifier(\n",
    "    n_estimators=2,    # daraxtlar soni\n",
    "    max_depth=5,       # maksimal chuqurlik\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Oâ€˜qitish\n",
    "rf_manual.fit(X_train, y_train)\n",
    "\n",
    "# Bashorat\n",
    "y_pred_manual = rf_manual.predict(X_test)\n",
    "\n",
    "# Baholash\n",
    "r2_manual = accuracy_score(y_test, y_pred_manual)\n",
    "print(\"ðŸŒ² Random Forest (Manual Search) Accuracy:\", r2_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ff5bac",
   "metadata": {},
   "source": [
    "#  DecisionTreeClassifier + Manual Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a4686e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ³ Decision Tree (Manual Search) Accuracy: 0.993798449612403\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# ðŸŒ³ Model â€” qoâ€˜lda tanlangan hyperparametrlar bilan\n",
    "dt_manual = DecisionTreeClassifier(\n",
    "    criterion=\"entropy\",   # yoki \"gini\"\n",
    "    max_depth=5,           # maksimal chuqurlik\n",
    "    min_samples_split=4,   # boâ€˜linish uchun minimal namunalar\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Oâ€˜qitish\n",
    "dt_manual.fit(X_train, y_train)\n",
    "\n",
    "# Bashorat\n",
    "y_pred_manual = dt_manual.predict(X_test)\n",
    "\n",
    "# Baholash\n",
    "acc_manual = accuracy_score(y_test, y_pred_manual)\n",
    "print(\"ðŸŒ³ Decision Tree (Manual Search) Accuracy:\", acc_manual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc17f43",
   "metadata": {},
   "source": [
    "#  LogisticRegression + Manual Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0924cf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Logistic Regression (Manual Search) Accuracy: 0.98656330749354\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "log_manual = LogisticRegression(\n",
    "    solver=\"liblinear\",   # kichik datasetlar uchun yaxshi\n",
    "    penalty=\"l2\",         # regularizatsiya turi (\"l1\" yoki \"l2\")\n",
    "    C=0.7,                # regularizatsiya kuchi (kichik boâ€˜lsa â€” koâ€˜proq jazo)\n",
    "    max_iter=500,         # iteratsiyalar soni\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Modelni oâ€˜qitish\n",
    "log_manual.fit(X_train, y_train)\n",
    "\n",
    "# Bashorat\n",
    "y_pred_manual = log_manual.predict(X_test)\n",
    "\n",
    "# Baholash\n",
    "acc_manual = accuracy_score(y_test, y_pred_manual)\n",
    "print(\"ðŸ“Š Logistic Regression (Manual Search) Accuracy:\", acc_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9449eca",
   "metadata": {},
   "source": [
    "#  XGBClassifier + Manual Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de833907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [10:55:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š XGBoost Classifier Accuracy: 0.9989664082687338\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Ma'lumotlarni tayyorlash\n",
    "X = df.drop(columns=\"cluster\")\n",
    "y = df[\"cluster\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# XGBoost modelini yaratish\n",
    "xgb_manual = XGBClassifier(\n",
    "    n_estimators=100,        # daraxtlar soni\n",
    "    learning_rate=0.1,       # oâ€˜rganish tezligi\n",
    "    max_depth=6,             # daraxt maksimal chuqurligi\n",
    "    reg_lambda=0.7,          # L2 regularizatsiya (LogisticRegressiondagi C ga oâ€˜xshash)\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "\n",
    "# Modelni oâ€˜qitish\n",
    "xgb_manual.fit(X_train, y_train)\n",
    "\n",
    "# Bashorat qilish\n",
    "y_pred_manual = xgb_manual.predict(X_test)\n",
    "\n",
    "# Baholash\n",
    "acc_manual = accuracy_score(y_test, y_pred_manual)\n",
    "print(\"ðŸ“Š XGBoost Classifier Accuracy:\", acc_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8703e96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Natijalar\n",
    "models = [\n",
    "    \"RandomForestClassifier\",\n",
    "    \"DecisionTreeClassifier\",\n",
    "    \"LogisticRegression\",\n",
    "    \"XGBClassifier\"\n",
    "]\n",
    "accuracy = [0.9330749354005168, 0.99334005168, 0.98656330, 0.9989664082687338]\n",
    "\n",
    "# Rasm hajmini sozlash\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Bar chart\n",
    "bars = plt.bar(models, accuracy, color=['skyblue', 'lightgreen', 'orange', 'pink'])\n",
    "\n",
    "# Accuracy qiymatlarni ustiga yozish\n",
    "for bar, acc in zip(bars, accuracy):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() - 0.02, \n",
    "             f\"{acc:.3f}\", ha='center', color='black', fontweight='bold')\n",
    "\n",
    "# Grafik sarlavhasi va o'qlar\n",
    "plt.title(\"Model Accuracy Comparison\", fontsize=16)\n",
    "plt.ylabel(\"Accuracy\", fontsize=14)\n",
    "plt.ylim(0, 1.05)  # y o'qi 0-1 orasida bo'lsin\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Faylni saqlash\n",
    "save_path = r\"C:\\Users\\Rasulbek907\\Desktop\\Project_MP\\Results\\Manual_Search_Tuning.jpg\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcd5c6b",
   "metadata": {},
   "source": [
    "# GRID SEARCH "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e464a",
   "metadata": {},
   "source": [
    "# RandomForestClassifier + Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "765f4e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "âœ… Eng yaxshi parametrlar:\n",
      "{'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "\n",
      "ðŸ“ˆ Random Forest (Grid Search) Accuracy: 0.9989664082687338\n",
      "\n",
      "ðŸ” Toâ€˜liq tahlil:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       774\n",
      "           1       1.00      1.00      1.00       774\n",
      "           2       1.00      1.00      1.00       774\n",
      "           3       1.00      1.00      1.00       774\n",
      "           4       1.00      1.00      1.00       774\n",
      "\n",
      "    accuracy                           1.00      3870\n",
      "   macro avg       1.00      1.00      1.00      3870\n",
      "weighted avg       1.00      1.00      1.00      3870\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# ðŸ” Grid Search uchun parametrlar toâ€˜plami\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],     # daraxtlar soni\n",
    "    'max_depth': [None, 5, 10, 15],     # daraxt chuqurligi\n",
    "    'min_samples_split': [2, 5, 10],    # boâ€˜linish uchun minimal namunalar\n",
    "    'min_samples_leaf': [1, 2, 4],      # barg tugunlari uchun minimal namunalar\n",
    "    'criterion': ['gini', 'entropy']    # ajratish mezoni\n",
    "}\n",
    "\n",
    "# ðŸ§  Grid Search sozlash\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                   # 5-fold cross validation\n",
    "    scoring='accuracy',     # aniqlikni baholash mezoni\n",
    "    n_jobs=-1,              # barcha yadroda bajarish\n",
    "    verbose=2               # jarayonni koâ€˜rsatish\n",
    ")\n",
    "\n",
    "# ðŸ”§ Modelni oâ€˜qitish (eng yaxshi parametrlarni topish)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ðŸ† Eng yaxshi model va parametrlar\n",
    "print(\"âœ… Eng yaxshi parametrlar:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Eng yaxshi modelni olish\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Bashorat\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# ðŸ“Š Natijalar\n",
    "print(\"\\nðŸ“ˆ Random Forest (Grid Search) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nðŸ” Toâ€˜liq tahlil:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abeda0e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fac57b1",
   "metadata": {},
   "source": [
    "#  DecisionTreeClassifier + Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f8539fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "âœ… Eng yaxshi parametrlar:\n",
      "{'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\n",
      "ðŸŒ³ Decision Tree (Grid Search) Accuracy: 0.9987080103359173\n",
      "\n",
      "ðŸ” Toâ€˜liq tahlil:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       774\n",
      "           1       1.00      1.00      1.00       774\n",
      "           2       1.00      1.00      1.00       774\n",
      "           3       1.00      1.00      1.00       774\n",
      "           4       1.00      1.00      1.00       774\n",
      "\n",
      "    accuracy                           1.00      3870\n",
      "   macro avg       1.00      1.00      1.00      3870\n",
      "weighted avg       1.00      1.00      1.00      3870\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# ðŸ” Grid Search uchun parametrlar toâ€˜plami\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # boâ€˜linish mezoni\n",
    "    'max_depth': [None, 5, 10, 20, 30],             # daraxt chuqurligi\n",
    "    'min_samples_split': [2, 5, 10],                # tugunni boâ€˜lish uchun minimal namunalar\n",
    "    'min_samples_leaf': [1, 2, 4],                  # barg tugun uchun eng kam namunalar\n",
    "    'splitter': ['best', 'random']                  # tugun ajratish usuli\n",
    "}\n",
    "\n",
    "# ðŸ§  Grid Search sozlash\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                   # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ðŸ”§ Modelni oâ€˜qitish\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ðŸ† Eng yaxshi parametrlar\n",
    "print(\"âœ… Eng yaxshi parametrlar:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Eng yaxshi modelni olish\n",
    "best_dt_model = grid_search.best_estimator_\n",
    "\n",
    "# Bashorat\n",
    "y_pred = best_dt_model.predict(X_test)\n",
    "\n",
    "# ðŸ“Š Natijalar\n",
    "print(\"\\nðŸŒ³ Decision Tree (Grid Search) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nðŸ” Toâ€˜liq tahlil:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0834aaeb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77b97406",
   "metadata": {},
   "source": [
    "#  Logistic Regression + Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fcdf310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "125 fits failed out of a total of 300.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 72, in _check_solver\n",
      "    raise ValueError(\n",
      "        f\"Only 'saga' solver supports elasticnet penalty, got solver={solver}.\"\n",
      "    )\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 77, in _check_solver\n",
      "    raise ValueError(\"penalty=None is not supported for the liblinear solver\")\n",
      "ValueError: penalty=None is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [       nan 0.97222222 0.92687339 0.98023256 0.97235142 0.93171835\n",
      "        nan        nan        nan 0.98856589        nan 0.93184755\n",
      "        nan 0.98404393 0.93113695 0.9874031  0.98016796 0.93184755\n",
      "        nan        nan        nan 0.98856589        nan 0.93184755\n",
      "        nan 0.98927649 0.93184755 0.98888889 0.98204134 0.93184755\n",
      "        nan        nan        nan 0.98856589        nan 0.93184755\n",
      "        nan 0.99024548 0.93184755 0.98934109 0.98217054 0.93184755\n",
      "        nan        nan        nan 0.98856589        nan 0.93184755\n",
      "        nan 0.99050388 0.93184755 0.98998708 0.98107235 0.93184755\n",
      "        nan        nan        nan 0.98856589        nan 0.93184755]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Eng yaxshi parametrlar:\n",
      "{'C': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "\n",
      "ðŸ“ˆ Logistic Regression (Grid Search) Accuracy: 0.99328165374677\n",
      "\n",
      "ðŸ” Toâ€˜liq tahlil:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       774\n",
      "           1       0.99      0.99      0.99       774\n",
      "           2       1.00      1.00      1.00       774\n",
      "           3       1.00      1.00      1.00       774\n",
      "           4       0.99      1.00      1.00       774\n",
      "\n",
      "    accuracy                           0.99      3870\n",
      "   macro avg       0.99      0.99      0.99      3870\n",
      "weighted avg       0.99      0.99      0.99      3870\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# ðŸ” Grid Search uchun parametrlar toâ€˜plami\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', None],  # jarima turi\n",
    "    'C': [0.01, 0.1, 1, 10, 100],                 # regularizatsiya kuchi\n",
    "    'solver': ['lbfgs', 'liblinear', 'saga'],     # optimallashtirish algoritmi\n",
    "}\n",
    "\n",
    "# ðŸ§  Grid Search sozlash\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=log_reg,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                   # 5-fold cross-validation\n",
    "    scoring='accuracy',     # aniqlik mezoni\n",
    "    n_jobs=-1,              # barcha CPU yadrolarida bajarish\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ðŸ”§ Oâ€˜qitish\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ðŸ† Eng yaxshi parametrlar\n",
    "print(\"âœ… Eng yaxshi parametrlar:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Eng yaxshi modelni olish\n",
    "best_log_model = grid_search.best_estimator_\n",
    "\n",
    "# Bashorat\n",
    "y_pred = best_log_model.predict(X_test)\n",
    "\n",
    "# ðŸ“Š Natijalar\n",
    "print(\"\\nðŸ“ˆ Logistic Regression (Grid Search) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nðŸ” Toâ€˜liq tahlil:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df69ffe7",
   "metadata": {},
   "source": [
    "# XGBClassifier + Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38141279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [12:27:20] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Eng yaxshi parametrlar:\n",
      "{'colsample_bytree': 0.6, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.6}\n",
      "\n",
      "ðŸ“ˆ XGBoost Classifier (Grid Search) Accuracy: 0.9997416020671834\n",
      "\n",
      "ðŸ” Toâ€˜liq tahlil:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       774\n",
      "           1       1.00      1.00      1.00       774\n",
      "           2       1.00      1.00      1.00       774\n",
      "           3       1.00      1.00      1.00       774\n",
      "           4       1.00      1.00      1.00       774\n",
      "\n",
      "    accuracy                           1.00      3870\n",
      "   macro avg       1.00      1.00      1.00      3870\n",
      "weighted avg       1.00      1.00      1.00      3870\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# XGBoost modelini yaratish\n",
    "xgb_clf = XGBClassifier(\n",
    "    use_label_encoder=False,  # XGBoost >=1.6 uchun kerak\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ðŸ” Grid Search uchun parametrlar toâ€˜plami\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],       # daraxtlar soni\n",
    "    'max_depth': [3, 5, 7],               # daraxt maksimal chuqurligi\n",
    "    'learning_rate': [0.01, 0.1, 0.2],   # oâ€˜rganish tezligi\n",
    "    'subsample': [0.6, 0.8, 1.0],        # har bir daraxt uchun ma'lumot ulushi\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0], # xususiyatlar ulushi\n",
    "    'gamma': [0, 0.1, 0.2]               # minimal loss reduction\n",
    "}\n",
    "\n",
    "# ðŸ§  Grid Search sozlash\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_clf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                   # 5-fold cross-validation\n",
    "    scoring='accuracy',     # aniqlik mezoni\n",
    "    n_jobs=-1,              # barcha CPU yadrolarida bajarish\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ðŸ”§ Oâ€˜qitish\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ðŸ† Eng yaxshi parametrlar\n",
    "print(\"âœ… Eng yaxshi parametrlar:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Eng yaxshi modelni olish\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Bashorat\n",
    "y_pred = best_xgb_model.predict(X_test)\n",
    "\n",
    "# ðŸ“Š Natijalar\n",
    "print(\"\\nðŸ“ˆ XGBoost Classifier (Grid Search) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nðŸ” Toâ€˜liq tahlil:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb292ba1",
   "metadata": {},
   "source": [
    "# RANDOM SEARCH "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f92bdb",
   "metadata": {},
   "source": [
    "# RandomForestClasssifier + Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "305a27bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "âœ… Eng yaxshi parametrlar:\n",
      "{'bootstrap': False, 'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 253}\n",
      "\n",
      "ðŸŒ² Random Forest (Random Search) Accuracy: 0.9987941429801895\n",
      "\n",
      "ðŸ” Toâ€˜liq tahlil:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      1161\n",
      "           1       0.99      1.00      1.00      1161\n",
      "           2       1.00      1.00      1.00      1161\n",
      "           3       1.00      1.00      1.00      1161\n",
      "           4       1.00      1.00      1.00      1161\n",
      "\n",
      "    accuracy                           1.00      5805\n",
      "   macro avg       1.00      1.00      1.00      5805\n",
      "weighted avg       1.00      1.00      1.00      5805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import randint\n",
    "\n",
    "# ðŸ”¹ Maâ€™lumotni ajratish\n",
    "X = df.drop(columns=\"cluster\")\n",
    "y = df[\"cluster\"]\n",
    "\n",
    "# ðŸ”¹ Train/Test boâ€˜lish\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ðŸŒ² Model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# ðŸŽ¯ Random Search uchun parametrlar diapazoni\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 300),         # daraxtlar soni\n",
    "    'max_depth': [None, 5, 10, 20, 30],       # chuqurlik\n",
    "    'min_samples_split': randint(2, 10),      # boâ€˜linish uchun minimal namunalar\n",
    "    'min_samples_leaf': randint(1, 5),        # barg uchun minimal namunalar\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # ajratish mezoni\n",
    "    'bootstrap': [True, False]                # bootstrap namunalar ishlatilsinmi\n",
    "}\n",
    "\n",
    "# âš™ï¸ Random Search sozlash\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,               # 20 ta tasodifiy kombinatsiyani sinaydi\n",
    "    cv=5,                    # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ðŸ”§ Modelni oâ€˜qitish\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# ðŸ† Eng yaxshi parametrlar\n",
    "print(\"âœ… Eng yaxshi parametrlar:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Eng yaxshi modelni olish\n",
    "best_rf_model = random_search.best_estimator_\n",
    "\n",
    "# Bashorat\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# ðŸ“Š Natijalar\n",
    "print(\"\\nðŸŒ² Random Forest (Random Search) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nðŸ” Toâ€˜liq tahlil:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e9ace",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc95aae3",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier + Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06d89519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "âœ… Eng yaxshi parametrlar:\n",
      "{'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 9, 'min_samples_split': 16, 'splitter': 'best'}\n",
      "\n",
      "ðŸŒ³ Decision Tree (Random Search) Accuracy: 0.9967269595176572\n",
      "\n",
      "ðŸ” Toâ€˜liq tahlil:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1161\n",
      "           1       0.99      0.99      0.99      1161\n",
      "           2       0.99      1.00      1.00      1161\n",
      "           3       1.00      1.00      1.00      1161\n",
      "           4       1.00      1.00      1.00      1161\n",
      "\n",
      "    accuracy                           1.00      5805\n",
      "   macro avg       1.00      1.00      1.00      5805\n",
      "weighted avg       1.00      1.00      1.00      5805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# ðŸ”¹ X va y ajratish\n",
    "X = df.drop(columns=\"cluster\")\n",
    "y = df[\"cluster\"]\n",
    "\n",
    "# ðŸ”¹ Train/Test boâ€˜lish\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ðŸŒ³ Model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# ðŸŽ¯ Random Search uchun parametrlar diapazoni\n",
    "param_dist = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],   # ajratish mezoni\n",
    "    'splitter': ['best', 'random'],                 # boâ€˜linish strategiyasi\n",
    "    'max_depth': [None, 5, 10, 20, 30, 50],         # daraxt chuqurligi\n",
    "    'min_samples_split': randint(2, 20),            # tugunni boâ€˜lish uchun minimal namunalar\n",
    "    'min_samples_leaf': randint(1, 10),             # barg uchun minimal namunalar\n",
    "    'max_features': [None, 'sqrt', 'log2']          # xususiyat tanlash strategiyasi\n",
    "}\n",
    "\n",
    "# âš™ï¸ Random Search sozlash\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=dt_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,               # 20 ta kombinatsiya sinovdan oâ€˜tadi\n",
    "    cv=5,                    # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ðŸ”§ Modelni oâ€˜qitish\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# ðŸ† Eng yaxshi parametrlar\n",
    "print(\"âœ… Eng yaxshi parametrlar:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Eng yaxshi modelni olish\n",
    "best_dt_model = random_search.best_estimator_\n",
    "\n",
    "# Bashorat\n",
    "y_pred = best_dt_model.predict(X_test)\n",
    "\n",
    "# ðŸ“Š Natijalar\n",
    "print(\"\\nðŸŒ³ Decision Tree (Random Search) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nðŸ” Toâ€˜liq tahlil:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92333f2",
   "metadata": {},
   "source": [
    "# Logistic regression + Random Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02e952a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "50 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 72, in _check_solver\n",
      "    raise ValueError(\n",
      "        f\"Only 'saga' solver supports elasticnet penalty, got solver={solver}.\"\n",
      "    )\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 77, in _check_solver\n",
      "    raise ValueError(\"penalty=None is not supported for the liblinear solver\")\n",
      "ValueError: penalty=None is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.92572905 0.97511997 0.97674419        nan\n",
      " 0.92779623        nan 0.92779623 0.98397933        nan        nan\n",
      "        nan 0.97511997 0.97511997 0.97511997        nan        nan\n",
      " 0.92779623        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Eng yaxshi parametrlar:\n",
      "{'C': np.float64(6.813075385877797), 'class_weight': None, 'fit_intercept': True, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "\n",
      "âš™ï¸ Logistic Regression (Random Search) Accuracy: 0.9848406546080964\n",
      "\n",
      "ðŸ” Toâ€˜liq tahlil:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97      1161\n",
      "           1       0.98      0.95      0.97      1161\n",
      "           2       0.98      1.00      0.99      1161\n",
      "           3       1.00      1.00      1.00      1161\n",
      "           4       0.99      1.00      1.00      1161\n",
      "\n",
      "    accuracy                           0.98      5805\n",
      "   macro avg       0.98      0.98      0.98      5805\n",
      "weighted avg       0.98      0.98      0.98      5805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# ðŸ”¹ X va y ajratish\n",
    "X = df.drop(columns=\"cluster\")\n",
    "y = df[\"cluster\"]\n",
    "\n",
    "# ðŸ”¹ Train/Test ajratish\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# âš™ï¸ Model\n",
    "log_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# ðŸŽ¯ Random Search uchun parametrlar diapazoni\n",
    "param_dist = {\n",
    "    'solver': ['liblinear', 'lbfgs', 'saga'],  # optimallashtirish algoritmlari\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', None],  # regularizatsiya turi\n",
    "    'C': uniform(0.01, 10),                     # regularizatsiya kuchi\n",
    "    'fit_intercept': [True, False],             # interceptni oâ€˜rganish yoki yoâ€˜q\n",
    "    'class_weight': [None, 'balanced']          # sinflarni balanslash\n",
    "}\n",
    "\n",
    "# ðŸ§  Random Search sozlash\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=log_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,               # 20 ta kombinatsiya sinovdan oâ€˜tadi\n",
    "    cv=5,                    # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ðŸ”§ Modelni oâ€˜qitish\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# ðŸ† Eng yaxshi parametrlar\n",
    "print(\"âœ… Eng yaxshi parametrlar:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Eng yaxshi modelni olish\n",
    "best_log_model = random_search.best_estimator_\n",
    "\n",
    "# Bashorat\n",
    "y_pred = best_log_model.predict(X_test)\n",
    "\n",
    "# ðŸ“Š Natijalar\n",
    "print(\"\\nâš™ï¸ Logistic Regression (Random Search) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nðŸ” Toâ€˜liq tahlil:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8109b7db",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b60c71",
   "metadata": {},
   "source": [
    "# Random Search +  XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86b700de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [14:22:18] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Eng yaxshi parametrlar:\n",
      "{'colsample_bytree': np.float64(0.6727299868828402), 'gamma': np.float64(0.07336180394137352), 'learning_rate': np.float64(0.1012726728878613), 'max_depth': 8, 'n_estimators': 285, 'reg_alpha': np.float64(0.023062425041415757), 'reg_lambda': np.float64(1.0495493205167783), 'subsample': np.float64(0.7599443886861021)}\n",
      "\n",
      "âš™ï¸ XGBoost Classifier (Random Search) Accuracy: 0.9987941429801895\n",
      "\n",
      "ðŸ” Toâ€˜liq tahlil:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1161\n",
      "           1       1.00      1.00      1.00      1161\n",
      "           2       1.00      1.00      1.00      1161\n",
      "           3       1.00      1.00      1.00      1161\n",
      "           4       1.00      1.00      1.00      1161\n",
      "\n",
      "    accuracy                           1.00      5805\n",
      "   macro avg       1.00      1.00      1.00      5805\n",
      "weighted avg       1.00      1.00      1.00      5805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# ðŸ”¹ X va y ajratish\n",
    "X = df.drop(columns=\"cluster\")\n",
    "y = df[\"cluster\"]\n",
    "\n",
    "# ðŸ”¹ Train/Test ajratish\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# âš™ï¸ XGBoost model\n",
    "xgb_model = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ðŸŽ¯ Random Search uchun parametrlar diapazoni\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 300),          # daraxtlar soni\n",
    "    'max_depth': randint(3, 10),               # daraxt chuqurligi\n",
    "    'learning_rate': uniform(0.01, 0.3),       # oâ€˜rganish tezligi\n",
    "    'subsample': uniform(0.6, 0.4),            # namunalar ulushi\n",
    "    'colsample_bytree': uniform(0.6, 0.4),     # xususiyatlar ulushi\n",
    "    'gamma': uniform(0, 0.4),                  # minimal loss kamayishi\n",
    "    'reg_lambda': uniform(0, 2),               # L2 regularizatsiya\n",
    "    'reg_alpha': uniform(0, 1)                 # L1 regularizatsiya\n",
    "}\n",
    "\n",
    "# ðŸ§  Random Search sozlash\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=25,               # 25 ta kombinatsiya sinovdan oâ€˜tadi\n",
    "    cv=5,                    # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ðŸ”§ Modelni oâ€˜qitish\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# ðŸ† Eng yaxshi parametrlar\n",
    "print(\"âœ… Eng yaxshi parametrlar:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Eng yaxshi modelni olish\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "# Bashorat\n",
    "y_pred = best_xgb_model.predict(X_test)\n",
    "\n",
    "# ðŸ“Š Natijalar\n",
    "print(\"\\nâš™ï¸ XGBoost Classifier (Random Search) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nðŸ” Toâ€˜liq tahlil:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b8540a",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb558a",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84b672c",
   "metadata": {},
   "source": [
    "# Bayesian Optimization +  RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b203dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "âœ… Eng yaxshi parametrlar:\n",
      "OrderedDict({'bootstrap': False, 'max_depth': 30, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300})\n",
      "\n",
      "ðŸŒ² Random Forest (Bayesian Optimization) Accuracy: 0.9987941429801895\n",
      "\n",
      "ðŸ” Toâ€˜liq tahlil:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      1161\n",
      "           1       1.00      1.00      1.00      1161\n",
      "           2       1.00      1.00      1.00      1161\n",
      "           3       1.00      1.00      1.00      1161\n",
      "           4       1.00      1.00      1.00      1161\n",
      "\n",
      "    accuracy                           1.00      5805\n",
      "   macro avg       1.00      1.00      1.00      5805\n",
      "weighted avg       1.00      1.00      1.00      5805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# ðŸŒ² Model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# ðŸŽ¯ Bayesian Optimization uchun parametrlar oraligâ€˜i\n",
    "param_space = {\n",
    "    'n_estimators': (50, 300),           # daraxtlar soni\n",
    "    'max_depth': (3, 30),                # chuqurlik\n",
    "    'min_samples_split': (2, 10),        # tugunni boâ€˜lish uchun minimal namunalar\n",
    "    'min_samples_leaf': (1, 5),          # barg uchun minimal namunalar\n",
    "    'max_features': ['sqrt', 'log2', None], # xususiyat tanlash usuli\n",
    "    'bootstrap': [True, False]           # bootstrap namunalar ishlatilsinmi\n",
    "}\n",
    "\n",
    "# âš™ï¸ BayesSearchCV sozlash\n",
    "opt = BayesSearchCV(\n",
    "    estimator=rf,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=25,              # necha kombinatsiya sinovdan oâ€˜tadi\n",
    "    cv=5,                   # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ðŸ”§ Modelni oâ€˜qitish\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "# ðŸ† Eng yaxshi parametrlar\n",
    "print(\"âœ… Eng yaxshi parametrlar:\")\n",
    "print(opt.best_params_)\n",
    "\n",
    "# Eng yaxshi model\n",
    "best_rf = opt.best_estimator_\n",
    "\n",
    "# Bashorat\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# ðŸ“Š Natijalar\n",
    "print(\"\\nðŸŒ² Random Forest (Bayesian Optimization) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nðŸ” Toâ€˜liq tahlil:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccda6821",
   "metadata": {},
   "source": [
    "# Bayesian Optimization +  DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e868fc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "âœ… Eng yaxshi parametrlar:\n",
      "OrderedDict({'criterion': 'entropy', 'max_depth': 18, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 20})\n",
      "\n",
      "ðŸŒ¿ Decision Tree (Bayesian Optimization) Accuracy: 0.9979328165374677\n",
      "\n",
      "ðŸ” Toâ€˜liq tahlil:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1161\n",
      "           1       1.00      0.99      0.99      1161\n",
      "           2       1.00      1.00      1.00      1161\n",
      "           3       1.00      1.00      1.00      1161\n",
      "           4       1.00      1.00      1.00      1161\n",
      "\n",
      "    accuracy                           1.00      5805\n",
      "   macro avg       1.00      1.00      1.00      5805\n",
      "weighted avg       1.00      1.00      1.00      5805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# ðŸ”¹ X va y ajratish\n",
    "X = df.drop(columns=\"cluster\")\n",
    "y = df[\"cluster\"]\n",
    "\n",
    "# ðŸ”¹ Train/Test ajratish\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ðŸŒ³ Model yaratish\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# ðŸŽ¯ Bayesian Optimization uchun parametrlar oraligâ€˜i\n",
    "param_space = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # qaror mezoni\n",
    "    'max_depth': (2, 30),                          # maksimal chuqurlik\n",
    "    'min_samples_split': (2, 20),                  # boâ€˜linish uchun minimal namunalar\n",
    "    'min_samples_leaf': (1, 10),                   # bargdagi minimal namunalar\n",
    "    'max_features': ['sqrt', 'log2', None]         # xususiyat tanlash strategiyasi\n",
    "}\n",
    "\n",
    "# âš™ï¸ BayesSearchCV sozlash\n",
    "opt = BayesSearchCV(\n",
    "    estimator=dt,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=25,              # sinovlar soni\n",
    "    cv=5,                   # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ðŸ”§ Modelni oâ€˜qitish\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "# ðŸ† Eng yaxshi parametrlar\n",
    "print(\"âœ… Eng yaxshi parametrlar:\")\n",
    "print(opt.best_params_)\n",
    "\n",
    "# Eng yaxshi modelni olish\n",
    "best_dt = opt.best_estimator_\n",
    "\n",
    "# Bashorat\n",
    "y_pred = best_dt.predict(X_test)\n",
    "\n",
    "# ðŸ“Š Natijalar\n",
    "print(\"\\nðŸŒ¿ Decision Tree (Bayesian Optimization) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nðŸ” Toâ€˜liq tahlil:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90df8f6e",
   "metadata": {},
   "source": [
    "# Bayesian Optimization +  LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e4d29f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "âœ… Eng yaxshi parametrlar: OrderedDict({'C': 0.5730813199999959, 'penalty': 'l2', 'solver': 'lbfgs'})\n",
      "\n",
      "ðŸ“ˆ Accuracy: 0.9900086132644272\n",
      "\n",
      "ðŸ” Toâ€˜liq tahlil:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      1161\n",
      "           1       0.99      0.97      0.98      1161\n",
      "           2       0.98      1.00      0.99      1161\n",
      "           3       1.00      1.00      1.00      1161\n",
      "           4       0.99      1.00      1.00      1161\n",
      "\n",
      "    accuracy                           0.99      5805\n",
      "   macro avg       0.99      0.99      0.99      5805\n",
      "weighted avg       0.99      0.99      0.99      5805\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# ðŸ”¹ X va y\n",
    "X = df.drop(columns=\"cluster\")\n",
    "y = df[\"cluster\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# âš™ï¸ Model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# ðŸ”§ Parametr oraligâ€˜i (mos juftliklar bilan)\n",
    "param_space = [\n",
    "    ({'solver': ['liblinear'], 'penalty': ['l1', 'l2'], 'C': (1e-4, 10.0, 'log-uniform')}),\n",
    "    ({'solver': ['lbfgs'], 'penalty': ['l2', None], 'C': (1e-4, 10.0, 'log-uniform')}),\n",
    "    ({'solver': ['saga'], 'penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "      'C': (1e-4, 10.0, 'log-uniform'), 'l1_ratio': (0, 1.0)})\n",
    "]\n",
    "\n",
    "# ðŸš€ Bayesian Optimization\n",
    "opt = BayesSearchCV(\n",
    "    estimator=log_reg,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=25,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "print(\"âœ… Eng yaxshi parametrlar:\", opt.best_params_)\n",
    "best_model = opt.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"\\nðŸ“ˆ Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nðŸ” Toâ€˜liq tahlil:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0101d8f3",
   "metadata": {},
   "source": [
    "# XGBoost Classifier â€” Bayesian Optimization (BayesSearchCV) bilan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3c5c5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [15:20:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Eng yaxshi parametrlar:\n",
      "OrderedDict({'colsample_bytree': 0.7291767706287129, 'gamma': 0.24143200071740095, 'learning_rate': 0.24182570891538493, 'max_depth': 12, 'n_estimators': 56, 'reg_alpha': 0.049482755709032725, 'reg_lambda': 0.6317490506523081, 'subsample': 0.7544902471216824})\n",
      "\n",
      "ðŸš€ XGBoost (Bayesian Optimization) Accuracy: 0.9987941429801895\n",
      "\n",
      "ðŸ” Toâ€˜liq tahlil:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1161\n",
      "           1       1.00      1.00      1.00      1161\n",
      "           2       1.00      1.00      1.00      1161\n",
      "           3       1.00      1.00      1.00      1161\n",
      "           4       1.00      1.00      1.00      1161\n",
      "\n",
      "    accuracy                           1.00      5805\n",
      "   macro avg       1.00      1.00      1.00      5805\n",
      "weighted avg       1.00      1.00      1.00      5805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "# ðŸ”¹ X va y ajratish\n",
    "X = df.drop(columns=\"cluster\")\n",
    "y = df[\"cluster\"]\n",
    "\n",
    "# ðŸ”¹ Train/Test boâ€˜lish\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# âš™ï¸ XGBoost model\n",
    "xgb = XGBClassifier(\n",
    "    eval_metric='mlogloss',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ðŸŽ¯ Bayesian Optimization uchun parametrlar oraligâ€˜i\n",
    "param_space = {\n",
    "    'n_estimators': Integer(50, 300),\n",
    "    'max_depth': Integer(3, 12),\n",
    "    'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "    'subsample': Real(0.6, 1.0),\n",
    "    'colsample_bytree': Real(0.6, 1.0),\n",
    "    'gamma': Real(0.0, 5.0),\n",
    "    'reg_alpha': Real(0.0, 1.0),     # L1 penalty\n",
    "    'reg_lambda': Real(0.0, 2.0),    # L2 penalty\n",
    "}\n",
    "\n",
    "# âš™ï¸ BayesSearchCV sozlash\n",
    "opt = BayesSearchCV(\n",
    "    estimator=xgb,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=25,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ðŸ”§ Modelni oâ€˜qitish\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "# ðŸ† Eng yaxshi parametrlar\n",
    "print(\"âœ… Eng yaxshi parametrlar:\")\n",
    "print(opt.best_params_)\n",
    "\n",
    "# ðŸŒŸ Eng yaxshi model\n",
    "best_xgb = opt.best_estimator_\n",
    "\n",
    "# Bashorat\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "# ðŸ“Š Natijalar\n",
    "print(\"\\nðŸš€ XGBoost (Bayesian Optimization) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nðŸ” Toâ€˜liq tahlil:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bfabbe",
   "metadata": {},
   "source": [
    "# Optuna (Automated / Advanced Method) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55c0c8",
   "metadata": {},
   "source": [
    "# Optuna + Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cda050f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\mpvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-11-18 15:21:58,813] A new study created in memory with name: no-name-66915eb3-f76b-4708-a27b-c3ca44b8656e\n",
      "[I 2025-11-18 15:22:11,858] Trial 0 finished with value: 0.9960132890365448 and parameters: {'n_estimators': 282, 'max_depth': 23, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 0 with value: 0.9960132890365448.\n",
      "[I 2025-11-18 15:22:15,913] Trial 1 finished with value: 0.9966039128829826 and parameters: {'n_estimators': 279, 'max_depth': 20, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.9966039128829826.\n",
      "[I 2025-11-18 15:22:19,218] Trial 2 finished with value: 0.9952011812476927 and parameters: {'n_estimators': 232, 'max_depth': 30, 'min_samples_split': 3, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.9966039128829826.\n",
      "[I 2025-11-18 15:22:25,483] Trial 3 finished with value: 0.9953488372093023 and parameters: {'n_estimators': 143, 'max_depth': 15, 'min_samples_split': 6, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 1 with value: 0.9966039128829826.\n",
      "[I 2025-11-18 15:22:26,657] Trial 4 finished with value: 0.9933554817275748 and parameters: {'n_estimators': 77, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 1 with value: 0.9966039128829826.\n",
      "[I 2025-11-18 15:22:28,562] Trial 5 finished with value: 0.9915097822074567 and parameters: {'n_estimators': 160, 'max_depth': 6, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.9966039128829826.\n",
      "[I 2025-11-18 15:22:32,729] Trial 6 finished with value: 0.996751568844592 and parameters: {'n_estimators': 285, 'max_depth': 17, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 6 with value: 0.996751568844592.\n",
      "[I 2025-11-18 15:22:36,283] Trial 7 finished with value: 0.9950535252860834 and parameters: {'n_estimators': 241, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 6 with value: 0.996751568844592.\n",
      "[I 2025-11-18 15:22:37,516] Trial 8 finished with value: 0.9771871539313399 and parameters: {'n_estimators': 138, 'max_depth': 2, 'min_samples_split': 3, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 6 with value: 0.996751568844592.\n",
      "[I 2025-11-18 15:22:39,585] Trial 9 finished with value: 0.9878183831672204 and parameters: {'n_estimators': 177, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 6 with value: 0.996751568844592.\n",
      "[I 2025-11-18 15:22:40,561] Trial 10 finished with value: 0.9968992248062015 and parameters: {'n_estimators': 52, 'max_depth': 15, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.9968992248062015.\n",
      "[I 2025-11-18 15:22:41,906] Trial 11 finished with value: 0.9970468807678111 and parameters: {'n_estimators': 81, 'max_depth': 14, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.9970468807678111.\n",
      "[I 2025-11-18 15:22:42,935] Trial 12 finished with value: 0.9968253968253968 and parameters: {'n_estimators': 57, 'max_depth': 13, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.9970468807678111.\n",
      "[I 2025-11-18 15:22:44,608] Trial 13 finished with value: 0.9969730527870063 and parameters: {'n_estimators': 99, 'max_depth': 12, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.9970468807678111.\n",
      "[I 2025-11-18 15:22:46,214] Trial 14 finished with value: 0.9947582133628646 and parameters: {'n_estimators': 97, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.9970468807678111.\n",
      "[I 2025-11-18 15:22:51,675] Trial 15 finished with value: 0.9960871170173496 and parameters: {'n_estimators': 110, 'max_depth': 11, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 11 with value: 0.9970468807678111.\n",
      "[I 2025-11-18 15:22:53,648] Trial 16 finished with value: 0.99734219269103 and parameters: {'n_estimators': 110, 'max_depth': 19, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 16 with value: 0.99734219269103.\n",
      "[I 2025-11-18 15:22:56,819] Trial 17 finished with value: 0.9969730527870063 and parameters: {'n_estimators': 192, 'max_depth': 19, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 16 with value: 0.99734219269103.\n",
      "[I 2025-11-18 15:22:58,681] Trial 18 finished with value: 0.9963086009597637 and parameters: {'n_estimators': 116, 'max_depth': 23, 'min_samples_split': 7, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 16 with value: 0.99734219269103.\n",
      "[I 2025-11-18 15:23:02,460] Trial 19 finished with value: 0.9942414174972315 and parameters: {'n_estimators': 77, 'max_depth': 21, 'min_samples_split': 7, 'min_samples_leaf': 10, 'max_features': None}. Best is trial 16 with value: 0.99734219269103.\n",
      "[I 2025-11-18 15:23:05,846] Trial 20 finished with value: 0.9968253968253968 and parameters: {'n_estimators': 205, 'max_depth': 27, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 16 with value: 0.99734219269103.\n",
      "[I 2025-11-18 15:23:07,399] Trial 21 finished with value: 0.9969730527870063 and parameters: {'n_estimators': 89, 'max_depth': 12, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 16 with value: 0.99734219269103.\n",
      "[I 2025-11-18 15:23:09,346] Trial 22 finished with value: 0.9971207087486157 and parameters: {'n_estimators': 121, 'max_depth': 17, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 16 with value: 0.99734219269103.\n",
      "[I 2025-11-18 15:23:11,385] Trial 23 finished with value: 0.9963824289405685 and parameters: {'n_estimators': 125, 'max_depth': 17, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 16 with value: 0.99734219269103.\n",
      "[I 2025-11-18 15:23:13,761] Trial 24 finished with value: 0.9971945367294204 and parameters: {'n_estimators': 148, 'max_depth': 18, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 16 with value: 0.99734219269103.\n",
      "[I 2025-11-18 15:23:16,268] Trial 25 finished with value: 0.9964562569213732 and parameters: {'n_estimators': 156, 'max_depth': 18, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 16 with value: 0.99734219269103.\n",
      "[I 2025-11-18 15:23:18,813] Trial 26 finished with value: 0.9966777408637874 and parameters: {'n_estimators': 165, 'max_depth': 22, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 16 with value: 0.99734219269103.\n",
      "[I 2025-11-18 15:23:20,960] Trial 27 finished with value: 0.9971945367294204 and parameters: {'n_estimators': 131, 'max_depth': 19, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 16 with value: 0.99734219269103.\n",
      "[I 2025-11-18 15:23:28,878] Trial 28 finished with value: 0.9963824289405685 and parameters: {'n_estimators': 139, 'max_depth': 25, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 16 with value: 0.99734219269103.\n",
      "[I 2025-11-18 15:23:31,991] Trial 29 finished with value: 0.9966777408637874 and parameters: {'n_estimators': 180, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 16 with value: 0.99734219269103.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¥‡ Eng yaxshi parametrlar:\n",
      "{'n_estimators': 110, 'max_depth': 19, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "âœ… Eng yaxshi aniqlik (CV): 0.9973\n",
      "ðŸŽ¯ Test aniqligi: 0.9981\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ðŸ”¹ X va y ajratish\n",
    "X = df.drop(columns=\"cluster\")\n",
    "y = df[\"cluster\"]\n",
    "\n",
    "# ðŸ”¹ Train/test boâ€˜lish (stratify bilan)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ðŸ”¹ Optuna maqsad funksiyasi\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 30)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # 3-fold cross-validation orqali aniqlikni oâ€˜lchash\n",
    "    score = cross_val_score(model, X_train, y_train, cv=3, scoring=\"accuracy\").mean()\n",
    "    return score\n",
    "\n",
    "# ðŸ”¹ Study yaratish\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# ðŸ”¹ Natijalar\n",
    "print(\"ðŸ¥‡ Eng yaxshi parametrlar:\")\n",
    "print(study.best_params)\n",
    "print(f\"âœ… Eng yaxshi aniqlik (CV): {study.best_value:.4f}\")\n",
    "\n",
    "# ðŸ”¹ Eng yaxshi modelni yaratish va testda sinovdan oâ€˜tkazish\n",
    "best_model = RandomForestClassifier(**study.best_params, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(f\"ðŸŽ¯ Test aniqligi: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e5bef4",
   "metadata": {},
   "source": [
    "# Optuna + Decision Tree  Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51ddc59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-18 15:24:06,291] A new study created in memory with name: no-name-0b55a4af-fbdc-4132-956c-2eaa9bcc009e\n",
      "[I 2025-11-18 15:24:06,666] Trial 0 finished with value: 0.9957179771133259 and parameters: {'criterion': 'entropy', 'max_depth': 12, 'min_samples_split': 5, 'min_samples_leaf': 5, 'splitter': 'best'}. Best is trial 0 with value: 0.9957179771133259.\n",
      "[I 2025-11-18 15:24:06,709] Trial 1 finished with value: 0.9884828349944629 and parameters: {'criterion': 'log_loss', 'max_depth': 29, 'min_samples_split': 17, 'min_samples_leaf': 9, 'splitter': 'random'}. Best is trial 0 with value: 0.9957179771133259.\n",
      "[I 2025-11-18 15:24:07,043] Trial 2 finished with value: 0.9943890734588409 and parameters: {'criterion': 'gini', 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 7, 'splitter': 'best'}. Best is trial 0 with value: 0.9957179771133259.\n",
      "[I 2025-11-18 15:24:07,078] Trial 3 finished with value: 0.6014027316352898 and parameters: {'criterion': 'log_loss', 'max_depth': 2, 'min_samples_split': 14, 'min_samples_leaf': 1, 'splitter': 'random'}. Best is trial 0 with value: 0.9957179771133259.\n",
      "[I 2025-11-18 15:24:07,127] Trial 4 finished with value: 0.9874492432631968 and parameters: {'criterion': 'log_loss', 'max_depth': 8, 'min_samples_split': 12, 'min_samples_leaf': 6, 'splitter': 'random'}. Best is trial 0 with value: 0.9957179771133259.\n",
      "[I 2025-11-18 15:24:07,502] Trial 5 finished with value: 0.9958656330749354 and parameters: {'criterion': 'entropy', 'max_depth': 26, 'min_samples_split': 8, 'min_samples_leaf': 9, 'splitter': 'best'}. Best is trial 5 with value: 0.9958656330749354.\n",
      "[I 2025-11-18 15:24:07,859] Trial 6 finished with value: 0.99328165374677 and parameters: {'criterion': 'gini', 'max_depth': 29, 'min_samples_split': 13, 'min_samples_leaf': 10, 'splitter': 'best'}. Best is trial 5 with value: 0.9958656330749354.\n",
      "[I 2025-11-18 15:24:07,916] Trial 7 finished with value: 0.992986341823551 and parameters: {'criterion': 'entropy', 'max_depth': 19, 'min_samples_split': 4, 'min_samples_leaf': 3, 'splitter': 'random'}. Best is trial 5 with value: 0.9958656330749354.\n",
      "[I 2025-11-18 15:24:08,329] Trial 8 finished with value: 0.9959394610557402 and parameters: {'criterion': 'entropy', 'max_depth': 28, 'min_samples_split': 20, 'min_samples_leaf': 7, 'splitter': 'best'}. Best is trial 8 with value: 0.9959394610557402.\n",
      "[I 2025-11-18 15:24:08,712] Trial 9 finished with value: 0.9968253968253968 and parameters: {'criterion': 'log_loss', 'max_depth': 14, 'min_samples_split': 8, 'min_samples_leaf': 1, 'splitter': 'best'}. Best is trial 9 with value: 0.9968253968253968.\n",
      "[I 2025-11-18 15:24:09,115] Trial 10 finished with value: 0.9964562569213732 and parameters: {'criterion': 'log_loss', 'max_depth': 19, 'min_samples_split': 8, 'min_samples_leaf': 2, 'splitter': 'best'}. Best is trial 9 with value: 0.9968253968253968.\n",
      "[I 2025-11-18 15:24:09,520] Trial 11 finished with value: 0.9968253968253968 and parameters: {'criterion': 'log_loss', 'max_depth': 19, 'min_samples_split': 8, 'min_samples_leaf': 1, 'splitter': 'best'}. Best is trial 9 with value: 0.9968253968253968.\n",
      "[I 2025-11-18 15:24:09,912] Trial 12 finished with value: 0.9967515688445922 and parameters: {'criterion': 'log_loss', 'max_depth': 15, 'min_samples_split': 8, 'min_samples_leaf': 3, 'splitter': 'best'}. Best is trial 9 with value: 0.9968253968253968.\n",
      "[I 2025-11-18 15:24:10,320] Trial 13 finished with value: 0.9968253968253968 and parameters: {'criterion': 'log_loss', 'max_depth': 22, 'min_samples_split': 9, 'min_samples_leaf': 1, 'splitter': 'best'}. Best is trial 9 with value: 0.9968253968253968.\n",
      "[I 2025-11-18 15:24:10,702] Trial 14 finished with value: 0.9960871170173496 and parameters: {'criterion': 'log_loss', 'max_depth': 11, 'min_samples_split': 5, 'min_samples_leaf': 4, 'splitter': 'best'}. Best is trial 9 with value: 0.9968253968253968.\n",
      "[I 2025-11-18 15:24:11,095] Trial 15 finished with value: 0.9966039128829826 and parameters: {'criterion': 'log_loss', 'max_depth': 22, 'min_samples_split': 10, 'min_samples_leaf': 1, 'splitter': 'best'}. Best is trial 9 with value: 0.9968253968253968.\n",
      "[I 2025-11-18 15:24:11,356] Trial 16 finished with value: 0.9928386858619417 and parameters: {'criterion': 'gini', 'max_depth': 6, 'min_samples_split': 15, 'min_samples_leaf': 3, 'splitter': 'best'}. Best is trial 9 with value: 0.9968253968253968.\n",
      "[I 2025-11-18 15:24:11,741] Trial 17 finished with value: 0.9968253968253968 and parameters: {'criterion': 'log_loss', 'max_depth': 15, 'min_samples_split': 6, 'min_samples_leaf': 2, 'splitter': 'best'}. Best is trial 9 with value: 0.9968253968253968.\n",
      "[I 2025-11-18 15:24:11,790] Trial 18 finished with value: 0.9934293097083794 and parameters: {'criterion': 'log_loss', 'max_depth': 18, 'min_samples_split': 11, 'min_samples_leaf': 4, 'splitter': 'random'}. Best is trial 9 with value: 0.9968253968253968.\n",
      "[I 2025-11-18 15:24:12,237] Trial 19 finished with value: 0.995422665190107 and parameters: {'criterion': 'gini', 'max_depth': 24, 'min_samples_split': 2, 'min_samples_leaf': 2, 'splitter': 'best'}. Best is trial 9 with value: 0.9968253968253968.\n",
      "[I 2025-11-18 15:24:12,661] Trial 20 finished with value: 0.9957179771133259 and parameters: {'criterion': 'log_loss', 'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 5, 'splitter': 'best'}. Best is trial 9 with value: 0.9968253968253968.\n",
      "[I 2025-11-18 15:24:13,042] Trial 21 finished with value: 0.9966039128829826 and parameters: {'criterion': 'log_loss', 'max_depth': 22, 'min_samples_split': 10, 'min_samples_leaf': 1, 'splitter': 'best'}. Best is trial 9 with value: 0.9968253968253968.\n",
      "[I 2025-11-18 15:24:13,417] Trial 22 finished with value: 0.9968253968253968 and parameters: {'criterion': 'log_loss', 'max_depth': 22, 'min_samples_split': 9, 'min_samples_leaf': 1, 'splitter': 'best'}. Best is trial 9 with value: 0.9968253968253968.\n",
      "[I 2025-11-18 15:24:13,810] Trial 23 finished with value: 0.9960132890365448 and parameters: {'criterion': 'log_loss', 'max_depth': 17, 'min_samples_split': 11, 'min_samples_leaf': 2, 'splitter': 'best'}. Best is trial 9 with value: 0.9968253968253968.\n",
      "[I 2025-11-18 15:24:14,202] Trial 24 finished with value: 0.9973421926910299 and parameters: {'criterion': 'log_loss', 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 1, 'splitter': 'best'}. Best is trial 24 with value: 0.9973421926910299.\n",
      "[I 2025-11-18 15:24:14,585] Trial 25 finished with value: 0.9969730527870063 and parameters: {'criterion': 'log_loss', 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 3, 'splitter': 'best'}. Best is trial 24 with value: 0.9973421926910299.\n",
      "[I 2025-11-18 15:24:14,638] Trial 26 finished with value: 0.9940199335548172 and parameters: {'criterion': 'log_loss', 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 4, 'splitter': 'random'}. Best is trial 24 with value: 0.9973421926910299.\n",
      "[I 2025-11-18 15:24:15,019] Trial 27 finished with value: 0.9969730527870063 and parameters: {'criterion': 'log_loss', 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 3, 'splitter': 'best'}. Best is trial 24 with value: 0.9973421926910299.\n",
      "[I 2025-11-18 15:24:15,404] Trial 28 finished with value: 0.9969730527870063 and parameters: {'criterion': 'entropy', 'max_depth': 24, 'min_samples_split': 2, 'min_samples_leaf': 3, 'splitter': 'best'}. Best is trial 24 with value: 0.9973421926910299.\n",
      "[I 2025-11-18 15:24:15,773] Trial 29 finished with value: 0.9945367294204503 and parameters: {'criterion': 'gini', 'max_depth': 16, 'min_samples_split': 4, 'min_samples_leaf': 6, 'splitter': 'best'}. Best is trial 24 with value: 0.9973421926910299.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ² Eng yaxshi parametrlar:\n",
      "{'criterion': 'log_loss', 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 1, 'splitter': 'best'}\n",
      "âœ… Eng yaxshi aniqlik (CV): 0.9973\n",
      "ðŸŽ¯ Test aniqligi: 0.9979\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ðŸ”¹ X va y\n",
    "X = df.drop(columns=\"cluster\")\n",
    "y = df[\"cluster\"]\n",
    "\n",
    "# ðŸ”¹ Train / Test boâ€˜lish\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ðŸ”¹ Optuna maqsad funksiyasi\n",
    "def objective(trial):\n",
    "    # Parametrlarni tanlash\n",
    "    criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\", \"log_loss\"])\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 30)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    splitter = trial.suggest_categorical(\"splitter\", [\"best\", \"random\"])\n",
    "\n",
    "    # Model yaratish\n",
    "    model = DecisionTreeClassifier(\n",
    "        criterion=criterion,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        splitter=splitter,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Cross-validation orqali baholash\n",
    "    score = cross_val_score(model, X_train, y_train, cv=3, scoring=\"accuracy\").mean()\n",
    "    return score\n",
    "\n",
    "# ðŸ”¹ Optuna study yaratish\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# ðŸ”¹ Natijalarni chiqarish\n",
    "print(\"ðŸŒ² Eng yaxshi parametrlar:\")\n",
    "print(study.best_params)\n",
    "print(f\"âœ… Eng yaxshi aniqlik (CV): {study.best_value:.4f}\")\n",
    "\n",
    "# ðŸ”¹ Eng yaxshi modelni testda sinash\n",
    "best_model = DecisionTreeClassifier(**study.best_params, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(f\"ðŸŽ¯ Test aniqligi: {accuracy_score(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9463bc",
   "metadata": {},
   "source": [
    "# Optuna + Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a95bbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-18 15:24:36,611] A new study created in memory with name: no-name-8f98b830-16b6-43ab-8ae6-b240775745d3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-18 15:24:36,676] Trial 0 finished with value: 0.9145071982281283 and parameters: {'criterion': 'entropy', 'splitter': 'random', 'max_depth': 20, 'min_samples_split': 18, 'min_samples_leaf': 12, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.9145071982281283.\n",
      "[I 2025-11-18 15:24:36,747] Trial 1 finished with value: 0.94499815430048 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 6, 'min_samples_split': 6, 'min_samples_leaf': 12, 'max_features': None}. Best is trial 1 with value: 0.94499815430048.\n",
      "[I 2025-11-18 15:24:37,452] Trial 2 finished with value: 0.9967515688445922 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 9, 'min_samples_split': 17, 'min_samples_leaf': 8, 'max_features': None}. Best is trial 2 with value: 0.9967515688445922.\n",
      "[I 2025-11-18 15:24:37,510] Trial 3 finished with value: 0.9548911037283132 and parameters: {'criterion': 'log_loss', 'splitter': 'random', 'max_depth': 22, 'min_samples_split': 20, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.9967515688445922.\n",
      "[I 2025-11-18 15:24:37,848] Trial 4 finished with value: 0.9833148763381322 and parameters: {'criterion': 'gini', 'splitter': 'best', 'max_depth': 3, 'min_samples_split': 17, 'min_samples_leaf': 8, 'max_features': None}. Best is trial 2 with value: 0.9967515688445922.\n",
      "[I 2025-11-18 15:24:37,904] Trial 5 finished with value: 0.9569582871908453 and parameters: {'criterion': 'entropy', 'splitter': 'random', 'max_depth': 22, 'min_samples_split': 18, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 2 with value: 0.9967515688445922.\n",
      "[I 2025-11-18 15:24:37,978] Trial 6 finished with value: 0.9874492432631967 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 21, 'min_samples_split': 12, 'min_samples_leaf': 16, 'max_features': None}. Best is trial 2 with value: 0.9967515688445922.\n",
      "[I 2025-11-18 15:24:38,700] Trial 7 finished with value: 0.9977113325950533 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 45, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:38,999] Trial 8 finished with value: 0.98390550018457 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 31, 'min_samples_split': 5, 'min_samples_leaf': 11, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:39,057] Trial 9 finished with value: 0.932373569582872 and parameters: {'criterion': 'entropy', 'splitter': 'random', 'max_depth': 47, 'min_samples_split': 18, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:39,366] Trial 10 finished with value: 0.9909191583610187 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 50, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:40,071] Trial 11 finished with value: 0.9957918050941306 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 36, 'min_samples_split': 11, 'min_samples_leaf': 19, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:40,791] Trial 12 finished with value: 0.9972683647102251 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 11, 'min_samples_split': 12, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:41,508] Trial 13 finished with value: 0.9971945367294204 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 40, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:42,246] Trial 14 finished with value: 0.9976375046142486 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 13, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:42,972] Trial 15 finished with value: 0.9976375046142486 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 15, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:43,298] Trial 16 finished with value: 0.9860465116279069 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:44,048] Trial 17 finished with value: 0.9976375046142486 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 41, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:44,806] Trial 18 finished with value: 0.9957918050941308 and parameters: {'criterion': 'entropy', 'splitter': 'best', 'max_depth': 15, 'min_samples_split': 4, 'min_samples_leaf': 16, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:45,019] Trial 19 finished with value: 0.9811738648947952 and parameters: {'criterion': 'gini', 'splitter': 'best', 'max_depth': 34, 'min_samples_split': 9, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:45,768] Trial 20 finished with value: 0.9975636766334439 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 44, 'min_samples_split': 14, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:46,506] Trial 21 finished with value: 0.9976375046142486 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 15, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:47,232] Trial 22 finished with value: 0.9976375046142486 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 15, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:47,955] Trial 23 finished with value: 0.9974160206718345 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 26, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:48,402] Trial 24 finished with value: 0.7983757844222961 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 2, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:49,137] Trial 25 finished with value: 0.9974160206718345 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 11, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:49,853] Trial 26 finished with value: 0.9972683647102251 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 17, 'min_samples_split': 6, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:50,568] Trial 27 finished with value: 0.996530084902178 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 25, 'min_samples_split': 9, 'min_samples_leaf': 9, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:50,789] Trial 28 finished with value: 0.8535252860834255 and parameters: {'criterion': 'gini', 'splitter': 'best', 'max_depth': 6, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:50,852] Trial 29 finished with value: 0.9523809523809523 and parameters: {'criterion': 'entropy', 'splitter': 'random', 'max_depth': 17, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:51,171] Trial 30 finished with value: 0.9711332595053526 and parameters: {'criterion': 'entropy', 'splitter': 'best', 'max_depth': 30, 'min_samples_split': 14, 'min_samples_leaf': 16, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:51,900] Trial 31 finished with value: 0.9976375046142486 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 41, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:52,634] Trial 32 finished with value: 0.9973421926910297 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 39, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:53,359] Trial 33 finished with value: 0.9974160206718345 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 47, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:53,438] Trial 34 finished with value: 0.988187523071244 and parameters: {'criterion': 'log_loss', 'splitter': 'random', 'max_depth': 43, 'min_samples_split': 9, 'min_samples_leaf': 14, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:54,171] Trial 35 finished with value: 0.9973421926910297 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 36, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:54,750] Trial 36 finished with value: 0.9940199335548172 and parameters: {'criterion': 'gini', 'splitter': 'best', 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:54,831] Trial 37 finished with value: 0.9955703211517166 and parameters: {'criterion': 'log_loss', 'splitter': 'random', 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:55,552] Trial 38 finished with value: 0.9974160206718345 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 19, 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:55,613] Trial 39 finished with value: 0.9452934662236988 and parameters: {'criterion': 'gini', 'splitter': 'random', 'max_depth': 24, 'min_samples_split': 11, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:55,944] Trial 40 finished with value: 0.9850867478774454 and parameters: {'criterion': 'entropy', 'splitter': 'best', 'max_depth': 46, 'min_samples_split': 13, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:56,678] Trial 41 finished with value: 0.9976375046142486 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 12, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:57,393] Trial 42 finished with value: 0.9975636766334441 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 6, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:58,129] Trial 43 finished with value: 0.9974898486526392 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:58,855] Trial 44 finished with value: 0.9973421926910297 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 19, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:24:59,578] Trial 45 finished with value: 0.9974160206718345 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 21, 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:25:00,325] Trial 46 finished with value: 0.9973421926910297 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 8, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:25:00,636] Trial 47 finished with value: 0.9790328534514581 and parameters: {'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 28, 'min_samples_split': 3, 'min_samples_leaf': 13, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:25:00,715] Trial 48 finished with value: 0.9964562569213731 and parameters: {'criterion': 'log_loss', 'splitter': 'random', 'max_depth': 32, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 7 with value: 0.9977113325950533.\n",
      "[I 2025-11-18 15:25:00,953] Trial 49 finished with value: 0.9844961240310077 and parameters: {'criterion': 'gini', 'splitter': 'best', 'max_depth': 23, 'min_samples_split': 7, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 7 with value: 0.9977113325950533.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Eng yaxshi parametrlar:\n",
      "{'criterion': 'log_loss', 'splitter': 'best', 'max_depth': 45, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': None}\n",
      "\n",
      "ðŸŒ³ Optuna + DecisionTreeClassifier Accuracy: 0.998\n",
      "\n",
      "ðŸ” Toâ€˜liq tahlil:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      1149\n",
      "           1       0.99      1.00      0.99      1173\n",
      "           2       1.00      1.00      1.00      1130\n",
      "           3       1.00      1.00      1.00      1195\n",
      "           4       1.00      1.00      1.00      1158\n",
      "\n",
      "    accuracy                           1.00      5805\n",
      "   macro avg       1.00      1.00      1.00      5805\n",
      "weighted avg       1.00      1.00      1.00      5805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Ma'lumotni bo'lish\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- Objective function for Optuna ---\n",
    "def objective(trial):\n",
    "    # Hiperparametrlar uchun qidiruv oralig'i\n",
    "    criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\", \"log_loss\"])\n",
    "    splitter = trial.suggest_categorical(\"splitter\", [\"best\", \"random\"])\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 50)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 20)\n",
    "    max_features = trial.suggest_categorical(\"max_features\", [None, \"sqrt\", \"log2\"])\n",
    "\n",
    "    # Model yaratish\n",
    "    model = DecisionTreeClassifier(\n",
    "        criterion=criterion,\n",
    "        splitter=splitter,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 5-fold cross-validation\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring=\"accuracy\").mean()\n",
    "    return score\n",
    "\n",
    "\n",
    "# --- Optuna optimization ---\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"âœ… Eng yaxshi parametrlar:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# --- Yakuniy modelni qurish ---\n",
    "best_params = study.best_params\n",
    "best_model = DecisionTreeClassifier(**best_params, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# --- Baholash ---\n",
    "y_pred = best_model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nðŸŒ³ Optuna + DecisionTreeClassifier Accuracy: {acc:.3f}\")\n",
    "print(\"\\nðŸ” Toâ€˜liq tahlil:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ccb625",
   "metadata": {},
   "source": [
    "# Optuna + XGBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c93db55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-18 15:25:06,430] A new study created in memory with name: no-name-854d2085-7181-4593-94a6-7a2571470ec3\n",
      "[I 2025-11-18 15:25:11,952] Trial 0 finished with value: 0.9977851605758582 and parameters: {'n_estimators': 270, 'max_depth': 4, 'learning_rate': 0.22513880449928927, 'subsample': 0.8498986864172584, 'colsample_bytree': 0.5964866809507954, 'gamma': 0.37556400745411833, 'min_child_weight': 7, 'reg_alpha': 1.3865486424070383, 'reg_lambda': 3.2220159192386477}. Best is trial 0 with value: 0.9977851605758582.\n",
      "[I 2025-11-18 15:25:18,165] Trial 1 finished with value: 0.9963086009597637 and parameters: {'n_estimators': 317, 'max_depth': 10, 'learning_rate': 0.26203357053744375, 'subsample': 0.7937718640662144, 'colsample_bytree': 0.9889807793105799, 'gamma': 4.776393685726201, 'min_child_weight': 7, 'reg_alpha': 0.28492936478403397, 'reg_lambda': 2.8434148427664634}. Best is trial 0 with value: 0.9977851605758582.\n",
      "[I 2025-11-18 15:25:23,405] Trial 2 finished with value: 0.9954964931709117 and parameters: {'n_estimators': 247, 'max_depth': 11, 'learning_rate': 0.08079152791710902, 'subsample': 0.7203299053363107, 'colsample_bytree': 0.9397055612655245, 'gamma': 2.9701247077728468, 'min_child_weight': 3, 'reg_alpha': 4.185715699627702, 'reg_lambda': 3.17114061508068}. Best is trial 0 with value: 0.9977851605758582.\n",
      "[I 2025-11-18 15:25:31,670] Trial 3 finished with value: 0.9943152454780361 and parameters: {'n_estimators': 418, 'max_depth': 4, 'learning_rate': 0.04694778098605086, 'subsample': 0.7056755046936982, 'colsample_bytree': 0.7154057398110606, 'gamma': 4.869589328889072, 'min_child_weight': 6, 'reg_alpha': 2.783905342737401, 'reg_lambda': 2.1147816639848265}. Best is trial 0 with value: 0.9977851605758582.\n",
      "[I 2025-11-18 15:25:42,189] Trial 4 finished with value: 0.9950535252860835 and parameters: {'n_estimators': 394, 'max_depth': 7, 'learning_rate': 0.022112963403518166, 'subsample': 0.6813676948457876, 'colsample_bytree': 0.7694556571086352, 'gamma': 4.742394300152105, 'min_child_weight': 5, 'reg_alpha': 1.3567863371616378, 'reg_lambda': 4.350378458290615}. Best is trial 0 with value: 0.9977851605758582.\n",
      "[I 2025-11-18 15:25:47,707] Trial 5 finished with value: 0.9942414174972315 and parameters: {'n_estimators': 293, 'max_depth': 4, 'learning_rate': 0.13221579719043994, 'subsample': 0.7104203425398984, 'colsample_bytree': 0.7856451845988994, 'gamma': 4.602391338563127, 'min_child_weight': 8, 'reg_alpha': 3.957915845740241, 'reg_lambda': 1.8306444712980952}. Best is trial 0 with value: 0.9977851605758582.\n",
      "[I 2025-11-18 15:25:53,140] Trial 6 finished with value: 0.995422665190107 and parameters: {'n_estimators': 249, 'max_depth': 12, 'learning_rate': 0.11635675727441988, 'subsample': 0.8713365773436053, 'colsample_bytree': 0.6412758237466523, 'gamma': 4.118068923175738, 'min_child_weight': 10, 'reg_alpha': 3.361463889617036, 'reg_lambda': 1.471564751062218}. Best is trial 0 with value: 0.9977851605758582.\n",
      "[I 2025-11-18 15:25:56,287] Trial 7 finished with value: 0.9963824289405684 and parameters: {'n_estimators': 130, 'max_depth': 6, 'learning_rate': 0.12905502547836575, 'subsample': 0.8124113183474044, 'colsample_bytree': 0.8599200862939449, 'gamma': 4.966106808709071, 'min_child_weight': 2, 'reg_alpha': 0.1435654895885713, 'reg_lambda': 1.4906226872720225}. Best is trial 0 with value: 0.9977851605758582.\n",
      "[I 2025-11-18 15:26:00,228] Trial 8 finished with value: 0.9980066445182724 and parameters: {'n_estimators': 158, 'max_depth': 12, 'learning_rate': 0.2765814292761558, 'subsample': 0.7077377695497802, 'colsample_bytree': 0.628463150909343, 'gamma': 0.06531215750407104, 'min_child_weight': 10, 'reg_alpha': 1.5122064515365452, 'reg_lambda': 4.601374992112558}. Best is trial 8 with value: 0.9980066445182724.\n",
      "[I 2025-11-18 15:26:03,458] Trial 9 finished with value: 0.9963086009597637 and parameters: {'n_estimators': 135, 'max_depth': 5, 'learning_rate': 0.1651083154644224, 'subsample': 0.7102728238608467, 'colsample_bytree': 0.5355414790893431, 'gamma': 2.601334115525438, 'min_child_weight': 8, 'reg_alpha': 0.9162000507513912, 'reg_lambda': 4.791354964373717}. Best is trial 8 with value: 0.9980066445182724.\n",
      "[I 2025-11-18 15:26:07,991] Trial 10 finished with value: 0.9976375046142486 and parameters: {'n_estimators': 171, 'max_depth': 15, 'learning_rate': 0.281057855049185, 'subsample': 0.5265727768805963, 'colsample_bytree': 0.5050666332062528, 'gamma': 0.05366684924381815, 'min_child_weight': 10, 'reg_alpha': 2.354755902715718, 'reg_lambda': 0.1474801975984601}. Best is trial 8 with value: 0.9980066445182724.\n",
      "[I 2025-11-18 15:26:13,595] Trial 11 finished with value: 0.9984496124031008 and parameters: {'n_estimators': 212, 'max_depth': 14, 'learning_rate': 0.22023863825087356, 'subsample': 0.9837326979406221, 'colsample_bytree': 0.6186819470220026, 'gamma': 0.04038934486129847, 'min_child_weight': 4, 'reg_alpha': 1.76106644662328, 'reg_lambda': 3.8057617895622995}. Best is trial 11 with value: 0.9984496124031008.\n",
      "[I 2025-11-18 15:26:17,987] Trial 12 finished with value: 0.9971945367294204 and parameters: {'n_estimators': 194, 'max_depth': 14, 'learning_rate': 0.2088502685700037, 'subsample': 0.9981996778720351, 'colsample_bytree': 0.6522617749232207, 'gamma': 1.2535832401126958, 'min_child_weight': 4, 'reg_alpha': 2.109582200563112, 'reg_lambda': 4.016950116367729}. Best is trial 11 with value: 0.9984496124031008.\n",
      "[I 2025-11-18 15:26:21,127] Trial 13 finished with value: 0.9966777408637875 and parameters: {'n_estimators': 102, 'max_depth': 13, 'learning_rate': 0.2195676943221418, 'subsample': 0.6001936468712438, 'colsample_bytree': 0.6032575120220075, 'gamma': 1.2223545462946777, 'min_child_weight': 1, 'reg_alpha': 1.7887638883638073, 'reg_lambda': 3.796754563491411}. Best is trial 11 with value: 0.9984496124031008.\n",
      "[I 2025-11-18 15:26:25,440] Trial 14 finished with value: 0.9971945367294204 and parameters: {'n_estimators': 199, 'max_depth': 8, 'learning_rate': 0.29974832061226087, 'subsample': 0.993407062883, 'colsample_bytree': 0.7194729822845158, 'gamma': 0.9557742391161423, 'min_child_weight': 4, 'reg_alpha': 2.887699666861077, 'reg_lambda': 4.880886646479374}. Best is trial 11 with value: 0.9984496124031008.\n",
      "[I 2025-11-18 15:26:32,559] Trial 15 finished with value: 0.9972683647102251 and parameters: {'n_estimators': 351, 'max_depth': 15, 'learning_rate': 0.1834041878995124, 'subsample': 0.8935109566594064, 'colsample_bytree': 0.5725567566654874, 'gamma': 2.023888751357932, 'min_child_weight': 5, 'reg_alpha': 0.8896736062223976, 'reg_lambda': 3.866135955398693}. Best is trial 11 with value: 0.9984496124031008.\n",
      "[I 2025-11-18 15:26:41,823] Trial 16 finished with value: 0.9972683647102251 and parameters: {'n_estimators': 498, 'max_depth': 12, 'learning_rate': 0.251763993748691, 'subsample': 0.6240081123052673, 'colsample_bytree': 0.6746775316932325, 'gamma': 0.6158243364461669, 'min_child_weight': 9, 'reg_alpha': 1.6001030717713787, 'reg_lambda': 3.5084095741378136}. Best is trial 11 with value: 0.9984496124031008.\n",
      "[I 2025-11-18 15:26:46,262] Trial 17 finished with value: 0.9968992248062015 and parameters: {'n_estimators': 222, 'max_depth': 9, 'learning_rate': 0.24662765029933773, 'subsample': 0.9255592220904753, 'colsample_bytree': 0.8014159366481189, 'gamma': 1.8610745321289048, 'min_child_weight': 3, 'reg_alpha': 3.418461811202313, 'reg_lambda': 4.476616553213887}. Best is trial 11 with value: 0.9984496124031008.\n",
      "[I 2025-11-18 15:26:49,788] Trial 18 finished with value: 0.9942414174972314 and parameters: {'n_estimators': 158, 'max_depth': 13, 'learning_rate': 0.19071014587621413, 'subsample': 0.7768121422438754, 'colsample_bytree': 0.5579910367326965, 'gamma': 3.48013897608199, 'min_child_weight': 6, 'reg_alpha': 4.850436312819717, 'reg_lambda': 2.5351016521055767}. Best is trial 11 with value: 0.9984496124031008.\n",
      "[I 2025-11-18 15:26:55,040] Trial 19 finished with value: 0.9984496124031008 and parameters: {'n_estimators': 212, 'max_depth': 10, 'learning_rate': 0.28989444809886467, 'subsample': 0.6402484621285257, 'colsample_bytree': 0.687641413161966, 'gamma': 0.052884932262528585, 'min_child_weight': 1, 'reg_alpha': 0.7512962239556982, 'reg_lambda': 4.968567211364357}. Best is trial 11 with value: 0.9984496124031008.\n",
      "[I 2025-11-18 15:27:01,199] Trial 20 finished with value: 0.9974898486526393 and parameters: {'n_estimators': 324, 'max_depth': 10, 'learning_rate': 0.29469350619847434, 'subsample': 0.5130702496882018, 'colsample_bytree': 0.7158856053521387, 'gamma': 1.6236193473026757, 'min_child_weight': 1, 'reg_alpha': 0.8478371997933625, 'reg_lambda': 4.087352522563815}. Best is trial 11 with value: 0.9984496124031008.\n",
      "[I 2025-11-18 15:27:06,390] Trial 21 finished with value: 0.998375784422296 and parameters: {'n_estimators': 219, 'max_depth': 11, 'learning_rate': 0.2688261197218093, 'subsample': 0.6435532956362664, 'colsample_bytree': 0.6385997319281045, 'gamma': 0.1358107446268346, 'min_child_weight': 2, 'reg_alpha': 0.6658894294031468, 'reg_lambda': 4.985616799692378}. Best is trial 11 with value: 0.9984496124031008.\n",
      "[I 2025-11-18 15:27:11,399] Trial 22 finished with value: 0.9977851605758582 and parameters: {'n_estimators': 229, 'max_depth': 10, 'learning_rate': 0.2365978178679573, 'subsample': 0.638389744759636, 'colsample_bytree': 0.6867012032647466, 'gamma': 0.6635026516619005, 'min_child_weight': 2, 'reg_alpha': 0.505522508982819, 'reg_lambda': 4.848270034306784}. Best is trial 11 with value: 0.9984496124031008.\n",
      "[I 2025-11-18 15:27:17,823] Trial 23 finished with value: 0.9985972683647102 and parameters: {'n_estimators': 279, 'max_depth': 8, 'learning_rate': 0.2647778530413521, 'subsample': 0.5818820577429307, 'colsample_bytree': 0.8257667179858942, 'gamma': 0.010170181943121553, 'min_child_weight': 2, 'reg_alpha': 0.72187630327259, 'reg_lambda': 4.292401181581978}. Best is trial 23 with value: 0.9985972683647102.\n",
      "[I 2025-11-18 15:27:23,499] Trial 24 finished with value: 0.9978589885566629 and parameters: {'n_estimators': 279, 'max_depth': 8, 'learning_rate': 0.1946907507737204, 'subsample': 0.5638409596450707, 'colsample_bytree': 0.8256536753476412, 'gamma': 0.7412203250470618, 'min_child_weight': 3, 'reg_alpha': 1.1471401194380586, 'reg_lambda': 4.332566012130461}. Best is trial 23 with value: 0.9985972683647102.\n",
      "[I 2025-11-18 15:27:30,823] Trial 25 finished with value: 0.9971207087486158 and parameters: {'n_estimators': 351, 'max_depth': 7, 'learning_rate': 0.24234326200976847, 'subsample': 0.5706069875360297, 'colsample_bytree': 0.8819348531561793, 'gamma': 1.1510004229673543, 'min_child_weight': 1, 'reg_alpha': 1.9517750169075696, 'reg_lambda': 3.3780150310423567}. Best is trial 23 with value: 0.9985972683647102.\n",
      "[I 2025-11-18 15:27:35,514] Trial 26 finished with value: 0.9983019564414913 and parameters: {'n_estimators': 181, 'max_depth': 9, 'learning_rate': 0.16453283979195193, 'subsample': 0.5632323029350321, 'colsample_bytree': 0.7460727164516012, 'gamma': 0.393303647810666, 'min_child_weight': 4, 'reg_alpha': 0.00013922789289089188, 'reg_lambda': 3.644880005735277}. Best is trial 23 with value: 0.9985972683647102.\n",
      "[I 2025-11-18 15:27:40,657] Trial 27 finished with value: 0.99734219269103 and parameters: {'n_estimators': 255, 'max_depth': 6, 'learning_rate': 0.277412758762621, 'subsample': 0.665479976044184, 'colsample_bytree': 0.9028092906506353, 'gamma': 1.5761715071825317, 'min_child_weight': 2, 'reg_alpha': 0.5337892127968309, 'reg_lambda': 4.180217896977756}. Best is trial 23 with value: 0.9985972683647102.\n",
      "[I 2025-11-18 15:27:45,138] Trial 28 finished with value: 0.9979328165374678 and parameters: {'n_estimators': 211, 'max_depth': 8, 'learning_rate': 0.21077671144097548, 'subsample': 0.9478589185389678, 'colsample_bytree': 0.8399481539561751, 'gamma': 0.42005641195689514, 'min_child_weight': 3, 'reg_alpha': 1.1340686585608466, 'reg_lambda': 0.4769348987405926}. Best is trial 23 with value: 0.9985972683647102.\n",
      "[I 2025-11-18 15:27:51,226] Trial 29 finished with value: 0.9980804724990773 and parameters: {'n_estimators': 278, 'max_depth': 3, 'learning_rate': 0.2205118343231465, 'subsample': 0.5936468781389793, 'colsample_bytree': 0.5977008125402614, 'gamma': 0.0031281596448106894, 'min_child_weight': 1, 'reg_alpha': 2.288598296337386, 'reg_lambda': 3.039467310285798}. Best is trial 23 with value: 0.9985972683647102.\n",
      "[I 2025-11-18 15:27:57,438] Trial 30 finished with value: 0.9964562569213731 and parameters: {'n_estimators': 327, 'max_depth': 14, 'learning_rate': 0.2315945752172499, 'subsample': 0.5358555851965092, 'colsample_bytree': 0.7480131550800537, 'gamma': 2.293447907128799, 'min_child_weight': 2, 'reg_alpha': 1.3034047355876293, 'reg_lambda': 4.548556115378591}. Best is trial 23 with value: 0.9985972683647102.\n",
      "[I 2025-11-18 15:28:02,642] Trial 31 finished with value: 0.9984496124031008 and parameters: {'n_estimators': 236, 'max_depth': 11, 'learning_rate': 0.2734633593876991, 'subsample': 0.6467235608900248, 'colsample_bytree': 0.6773514240493087, 'gamma': 0.20142388952878384, 'min_child_weight': 2, 'reg_alpha': 0.608129647251207, 'reg_lambda': 4.993153683746646}. Best is trial 23 with value: 0.9985972683647102.\n",
      "[I 2025-11-18 15:28:08,183] Trial 32 finished with value: 0.9980066445182724 and parameters: {'n_estimators': 259, 'max_depth': 11, 'learning_rate': 0.26128971698541376, 'subsample': 0.8271311343748904, 'colsample_bytree': 0.6767760894151156, 'gamma': 0.398723398357946, 'min_child_weight': 1, 'reg_alpha': 0.282580799813896, 'reg_lambda': 4.6290158600841735}. Best is trial 23 with value: 0.9985972683647102.\n",
      "[I 2025-11-18 15:28:12,832] Trial 33 finished with value: 0.9977851605758582 and parameters: {'n_estimators': 228, 'max_depth': 10, 'learning_rate': 0.2935595046228291, 'subsample': 0.7666556847867839, 'colsample_bytree': 0.955896539725271, 'gamma': 0.8836219951319441, 'min_child_weight': 3, 'reg_alpha': 0.5381032443516528, 'reg_lambda': 4.161139408033624}. Best is trial 23 with value: 0.9985972683647102.\n",
      "[I 2025-11-18 15:28:18,633] Trial 34 finished with value: 0.9977113325950535 and parameters: {'n_estimators': 298, 'max_depth': 9, 'learning_rate': 0.26561087804703243, 'subsample': 0.74428887810748, 'colsample_bytree': 0.6967037138115157, 'gamma': 0.5364697490813919, 'min_child_weight': 4, 'reg_alpha': 1.7202843593810127, 'reg_lambda': 2.748838569381336}. Best is trial 23 with value: 0.9985972683647102.\n",
      "[I 2025-11-18 15:28:23,693] Trial 35 finished with value: 0.998080472499077 and parameters: {'n_estimators': 241, 'max_depth': 11, 'learning_rate': 0.2584075326357385, 'subsample': 0.6733040264672205, 'colsample_bytree': 0.8041520875500504, 'gamma': 0.26092624767384853, 'min_child_weight': 2, 'reg_alpha': 1.1133224718682606, 'reg_lambda': 4.986148379515328}. Best is trial 23 with value: 0.9985972683647102.\n",
      "[I 2025-11-18 15:28:27,948] Trial 36 finished with value: 0.9964562569213733 and parameters: {'n_estimators': 201, 'max_depth': 13, 'learning_rate': 0.2834433537907578, 'subsample': 0.6106379565697272, 'colsample_bytree': 0.6151226716833604, 'gamma': 0.9639805575235942, 'min_child_weight': 3, 'reg_alpha': 2.5990803839808274, 'reg_lambda': 4.421890186608069}. Best is trial 23 with value: 0.9985972683647102.\n",
      "[I 2025-11-18 15:28:35,763] Trial 37 finished with value: 0.9978589885566629 and parameters: {'n_estimators': 273, 'max_depth': 7, 'learning_rate': 0.07501321941611655, 'subsample': 0.6574816428953465, 'colsample_bytree': 0.7671188208813353, 'gamma': 0.2466507854127028, 'min_child_weight': 5, 'reg_alpha': 1.3580724165059581, 'reg_lambda': 3.7118997329400902}. Best is trial 23 with value: 0.9985972683647102.\n",
      "[I 2025-11-18 15:28:43,025] Trial 38 finished with value: 0.9966039128829827 and parameters: {'n_estimators': 398, 'max_depth': 10, 'learning_rate': 0.23362784378670612, 'subsample': 0.7456926520826562, 'colsample_bytree': 0.6573738848527054, 'gamma': 3.7412650144457498, 'min_child_weight': 6, 'reg_alpha': 0.23145856712403773, 'reg_lambda': 3.1296046734284877}. Best is trial 23 with value: 0.9985972683647102.\n",
      "[I 2025-11-18 15:28:48,920] Trial 39 finished with value: 0.9966039128829826 and parameters: {'n_estimators': 310, 'max_depth': 14, 'learning_rate': 0.25093426559459414, 'subsample': 0.5811123605483005, 'colsample_bytree': 0.7313037128974074, 'gamma': 3.1182303367537907, 'min_child_weight': 1, 'reg_alpha': 0.7698070560937056, 'reg_lambda': 2.113701624838106}. Best is trial 23 with value: 0.9985972683647102.\n",
      "[I 2025-11-18 15:28:55,523] Trial 40 finished with value: 0.9948320413436693 and parameters: {'n_estimators': 134, 'max_depth': 12, 'learning_rate': 0.016773408073913115, 'subsample': 0.5393948360199025, 'colsample_bytree': 0.7788053007305032, 'gamma': 0.4813552485308148, 'min_child_weight': 2, 'reg_alpha': 0.39928490992606375, 'reg_lambda': 4.665703203160084}. Best is trial 23 with value: 0.9985972683647102.\n",
      "[I 2025-11-18 15:29:01,638] Trial 41 finished with value: 0.9985234403839055 and parameters: {'n_estimators': 239, 'max_depth': 11, 'learning_rate': 0.2719142346799468, 'subsample': 0.640403998838492, 'colsample_bytree': 0.6246150188690595, 'gamma': 0.009994352498994287, 'min_child_weight': 2, 'reg_alpha': 0.6921845394136164, 'reg_lambda': 4.968554587431485}. Best is trial 23 with value: 0.9985972683647102.\n",
      "[I 2025-11-18 15:29:07,052] Trial 42 finished with value: 0.9978589885566629 and parameters: {'n_estimators': 250, 'max_depth': 11, 'learning_rate': 0.28675565567054145, 'subsample': 0.6867040116587214, 'colsample_bytree': 0.5753178093467757, 'gamma': 0.24105470317704014, 'min_child_weight': 3, 'reg_alpha': 1.022607240917001, 'reg_lambda': 4.358539224306131}. Best is trial 23 with value: 0.9985972683647102.\n",
      "[I 2025-11-18 15:29:12,184] Trial 43 finished with value: 0.9986710963455151 and parameters: {'n_estimators': 181, 'max_depth': 8, 'learning_rate': 0.2740115715645152, 'subsample': 0.627096561571892, 'colsample_bytree': 0.6253292401690854, 'gamma': 0.0038683166138770778, 'min_child_weight': 2, 'reg_alpha': 0.015627519934513834, 'reg_lambda': 4.729575293777444}. Best is trial 43 with value: 0.9986710963455151.\n",
      "[I 2025-11-18 15:29:16,220] Trial 44 finished with value: 0.9976375046142488 and parameters: {'n_estimators': 180, 'max_depth': 8, 'learning_rate': 0.26333745282906257, 'subsample': 0.6178997798469749, 'colsample_bytree': 0.5368572655931371, 'gamma': 0.7537856528627889, 'min_child_weight': 1, 'reg_alpha': 0.035542033104950854, 'reg_lambda': 4.7055218271630705}. Best is trial 43 with value: 0.9986710963455151.\n",
      "[I 2025-11-18 15:29:21,011] Trial 45 finished with value: 0.9983019564414913 and parameters: {'n_estimators': 167, 'max_depth': 6, 'learning_rate': 0.14095507745386673, 'subsample': 0.7256320370619492, 'colsample_bytree': 0.618160190675918, 'gamma': 0.040652663175851186, 'min_child_weight': 7, 'reg_alpha': 0.23152119306439234, 'reg_lambda': 3.8835267970091145}. Best is trial 43 with value: 0.9986710963455151.\n",
      "[I 2025-11-18 15:29:24,115] Trial 46 finished with value: 0.9971945367294206 and parameters: {'n_estimators': 144, 'max_depth': 9, 'learning_rate': 0.29953960011778225, 'subsample': 0.6840848746397288, 'colsample_bytree': 0.6558437546347764, 'gamma': 1.090366164191815, 'min_child_weight': 4, 'reg_alpha': 1.4605416510121065, 'reg_lambda': 4.172242532138418}. Best is trial 43 with value: 0.9986710963455151.\n",
      "[I 2025-11-18 15:29:29,243] Trial 47 finished with value: 0.9977113325950535 and parameters: {'n_estimators': 194, 'max_depth': 7, 'learning_rate': 0.2031729982458952, 'subsample': 0.6241766476753102, 'colsample_bytree': 0.5762780823634123, 'gamma': 0.009496212953623937, 'min_child_weight': 2, 'reg_alpha': 3.0397621545610534, 'reg_lambda': 4.693993279301581}. Best is trial 43 with value: 0.9986710963455151.\n",
      "[I 2025-11-18 15:29:31,712] Trial 48 finished with value: 0.9965300849021779 and parameters: {'n_estimators': 112, 'max_depth': 5, 'learning_rate': 0.2849953408701199, 'subsample': 0.5037363548261898, 'colsample_bytree': 0.6978942552332731, 'gamma': 1.4778174779375746, 'min_child_weight': 3, 'reg_alpha': 1.9167358348372368, 'reg_lambda': 4.347359840898186}. Best is trial 43 with value: 0.9986710963455151.\n",
      "[I 2025-11-18 15:29:41,357] Trial 49 finished with value: 0.9952750092284977 and parameters: {'n_estimators': 464, 'max_depth': 9, 'learning_rate': 0.09275275960732873, 'subsample': 0.5511275297560337, 'colsample_bytree': 0.5240980678028828, 'gamma': 4.32471084580145, 'min_child_weight': 1, 'reg_alpha': 0.769139059728874, 'reg_lambda': 1.1659375798408091}. Best is trial 43 with value: 0.9986710963455151.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Eng yaxshi parametrlar:\n",
      "{'n_estimators': 181, 'max_depth': 8, 'learning_rate': 0.2740115715645152, 'subsample': 0.627096561571892, 'colsample_bytree': 0.6253292401690854, 'gamma': 0.0038683166138770778, 'min_child_weight': 2, 'reg_alpha': 0.015627519934513834, 'reg_lambda': 4.729575293777444}\n",
      "\n",
      "ðŸ”¥ Optuna + XGBoostClassifier Accuracy: 0.9976\n",
      "\n",
      "ðŸ” Toâ€˜liq tahlil:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      1149\n",
      "           1       0.99      1.00      1.00      1173\n",
      "           2       1.00      1.00      1.00      1130\n",
      "           3       1.00      1.00      1.00      1195\n",
      "           4       1.00      1.00      1.00      1158\n",
      "\n",
      "    accuracy                           1.00      5805\n",
      "   macro avg       1.00      1.00      1.00      5805\n",
      "weighted avg       1.00      1.00      1.00      5805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Ma'lumotni bo'lish\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# --- Objective function for Optuna ---\n",
    "def objective(trial):\n",
    "\n",
    "    # XGBoost hiperparametr qidiruv oralig'i\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 5.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 5.0),\n",
    "        \"objective\": \"multi:softmax\" if len(set(y)) > 2 else \"binary:logistic\",\n",
    "        \"num_class\": len(set(y)) if len(set(y)) > 2 else None,\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"random_state\": 42,\n",
    "        \"tree_method\": \"hist\"  # tezroq\n",
    "    }\n",
    "\n",
    "    # Model\n",
    "    model = XGBClassifier(**params)\n",
    "\n",
    "    # 5-fold CV\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring=\"accuracy\").mean()\n",
    "    return score\n",
    "\n",
    "\n",
    "# --- Optuna optimization ---\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"âœ… Eng yaxshi parametrlar:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# --- Final model ---\n",
    "best_params = study.best_params\n",
    "best_model = XGBClassifier(\n",
    "    **best_params,\n",
    "    objective=\"multi:softmax\" if len(set(y)) > 2 else \"binary:logistic\",\n",
    "    num_class=len(set(y)) if len(set(y)) > 2 else None,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\"\n",
    ")\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# --- Baholash ---\n",
    "y_pred = best_model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nðŸ”¥ Optuna + XGBoostClassifier Accuracy: {acc:.4f}\")\n",
    "print(\"\\nðŸ” Toâ€˜liq tahlil:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad74319d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
